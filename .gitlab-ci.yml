variables:
  PIPELINE_TOOLBOX_IMAGE: "registry.gitlab.com/northern.tech/mender/mender-test-containers:aws-k8s-v1"
  EKS_CLUSTER_NAME: "helmci-${CI_PIPELINE_ID}"

stages:
  - build
  - test
  - publish

include:
  - project: 'Northern.tech/Mender/mendertesting'
    file: '.gitlab-ci-check-commits-signoffs.yml'
  - project: 'Northern.tech/Mender/mendertesting'
    file: '.gitlab-ci-check-license.yml'
  - project: 'Northern.tech/Mender/mendertesting'
    file: '.gitlab-ci-github-status-updates.yml'

.set_eks_helmci_vars: &set_eks_helmci_vars
  - export AWS_ACCESS_KEY_ID=$CI_EKSCTL_ACCESS_KEY_ID_TEST
  - export AWS_SECRET_ACCESS_KEY=$CI_EKSCTL_AWS_SECRET_ACCESS_KEY_TEST
  - export AWS_DEFAULT_REGION=eu-central-1

build:setup_eks_cluster:
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: .pre
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  before_script:
  script:
    - *set_eks_helmci_vars
    - |
      echo "INFO - Temporary EKS cluster setup"
      eksctl create cluster \
        --name ${EKS_CLUSTER_NAME} \
        --tags "env=helm-ci-test" \
        --asg-access \
        --spot \
        --instance-types m6a.xlarge,t3.xlarge \
        --nodes 4
    - echo "INFO - assigning SSO roles for debugging failed tests:"
    - eksctl create iamidentitymapping --cluster ${EKS_CLUSTER_NAME} --arn arn:aws:iam::017659451055:role/AWSReservedSSO_AdministratorAccess_d80c0803237d713a --username "admin:{{SessionName}}" --group "system:masters" --no-duplicate-arns
    - eksctl create iamidentitymapping --cluster ${EKS_CLUSTER_NAME} --arn arn:aws:iam::017659451055:role/AWSReservedSSO_PowerUserAccess_28a0f819b31196fb --username "admin:{{SessionName}}" --group "system:masters" --no-duplicate-arns
    - eksctl create iamidentitymapping --cluster ${EKS_CLUSTER_NAME} --arn arn:aws:iam::017659451055:role/AWSReservedSSO_EKSAccess_8c2574e21d9fc21c --username "admin:{{SessionName}}" --group "system:masters" --no-duplicate-arns
    - echo "DEBUG - kubectl cluster version:"
    - kubectl version

build:
  stage: build
  image: debian:buster
  before_script:
    - apt-get update -y
    - apt-get install -y curl make
    - curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | DESIRED_VERSION="v3.8.2" bash
  script:
    - make lint
    - make package
  artifacts:
    expire_in: 2w
    paths:
      - mender-*.tgz

.get_kubectl_and_tools: &get_kubectl_and_tools
  # Install kubectl
  - apt update && apt install -yyq curl
  - curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.6/bin/linux/amd64/kubectl
  - chmod +x ./kubectl
  - mv ./kubectl /usr/local/bin/kubectl
  # Install AWS CLI and aws-iam-authenticator
  - apt install -yyq awscli
  - curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator
  - chmod +x ./aws-iam-authenticator
  - mv ./aws-iam-authenticator /usr/local/bin/aws-iam-authenticator
  # Install kubectx
  - apt install -yyq kubectx

.setup_eks_cluster_staging: &setup_eks_cluster_staging
  # Configure AWS CLI for staging cluster
  - export AWS_ACCESS_KEY_ID=$CI_JOBS_AWS_ACCESS_KEY_ID_STAGING
  - export AWS_SECRET_ACCESS_KEY=$CI_JOBS_AWS_SECRET_ACCESS_KEY_STAGING
  - export AWS_DEFAULT_REGION=$CI_JOBS_AWS_REGION_STAGING
  - aws eks --region $CI_JOBS_AWS_REGION_STAGING update-kubeconfig --name $CI_JOBS_AWS_EKS_CLUSTER_NAME_STAGING
  - kubectl config set-context --current --namespace=mender-helm-tests

.setup_s3_helm_chart_repo: &setup_s3_helm_chart_repo
  # Configure AWS CLI for the S3 repository
  - export AWS_ACCESS_KEY_ID=$S3_HELM_CHART_REPO_AWS_ACCESS_KEY_ID
  - export AWS_SECRET_ACCESS_KEY=$S3_HELM_CHART_REPO_AWS_SECRET_ACCESS_KEY
  - export AWS_DEFAULT_REGION=$S3_HELM_CHART_REPO_AWS_REGION
  - export AWS_S3_SSE=AES256

test:helm_chart_install:
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  stage: test
  dependencies:
    - build
  image: debian:buster
  before_script:
    - *get_kubectl_and_tools
    - *setup_eks_cluster_staging
    # Install test dependencies
    - apt install -yyq make uuid-runtime
    - tests/ci-deps-k8s.sh
    # Clean possible leftovers from an unfinished run
    - tests/ci-test-teardown.sh || true
    - tests/ci-make-clean.sh || true
  script:
    - tests/ci-make-deps.sh
    - tests/ci-make-helm.sh
    - make test
  after_script:
    - *setup_eks_cluster_staging
    - tests/ci-make-clean.sh

publish:helm_chart_publishing:
  rules:
    - if: '$CI_COMMIT_TAG'
      changes:
        - mender/Chart.yaml
      when: manual
  stage: publish
  dependencies:
    - build
  image: debian:buster
  before_script:
    - apt-get update -y
    - apt-get install -y curl make git
    - curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | DESIRED_VERSION="v3.8.2" bash
    - *get_kubectl_and_tools
    - helm plugin install https://github.com/hypnoglow/helm-s3.git --version 0.14.0
    - *setup_s3_helm_chart_repo
  script:
    - helm repo add mender s3://${S3_HELM_CHART_REPO}
    - helm s3 push --acl="public-read" --relative --timeout=60s ./mender-*.tgz mender
    - aws cloudfront create-invalidation --distribution-id ${S3_HELM_CHART_CDN_DISTRIBUTION_ID} --paths "/*"

.eks_cleanup: &eks_cleanup
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: .post
  allow_failure: true # can't find a way to avoid eks cleanup when no eks is setup, so let's allow to fail for now
  dependencies:
    - build:setup_eks_cluster
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  before_script:
    - *set_eks_helmci_vars
  script:
    - echo "INFO - deleting the temporary EKS cluster"
    - eksctl delete cluster -n ${EKS_CLUSTER_NAME}

cleanup_eks_cluster:failed:
  when: on_failure
  <<: *eks_cleanup

cleanup_eks_cluster:success:
  when: on_success
  <<: *eks_cleanup

cleanup_eks_cluster:failed:manual:
  when: manual
  <<: *eks_cleanup
